{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "comment_bug_relation_processing_simple_ast.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3B4YT6_dVsP"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import json\n",
        "from tqdm import tqdm as tq\n",
        "import pickle"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdktH0U-dVsU",
        "outputId": "985c5897-fa2a-45bb-a1d8-29950c1d3cc6"
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "from gensim import models\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLJXzqVNdVsY"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "      if (token not in gensim.parsing.preprocessing.STOPWORDS and len(token)) > 3:\n",
        "        result.append(lemmatize_stemming(token))\n",
        "    \n",
        "    return result"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKlyhp52dVsZ"
      },
      "source": [
        "def camel_case_split(identifier):\n",
        "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
        "    return [m.group(0) for m in matches]\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "def normalizeString(s):\n",
        "    # s = unicodeToAscii(s.lower().strip())\n",
        "    s = unicodeToAscii(s.strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz0o-6m9dVsa"
      },
      "source": [
        "def parse_header(header):\n",
        "    header = normalizeString(header)\n",
        "    token_list = []\n",
        "    for token in header.split(' '):\n",
        "        token_list += camel_case_split(token)\n",
        "    # token_list = set(token_list)\n",
        "    norm_list = []\n",
        "    for token in token_list:\n",
        "        norm_list.append(lemmatize_stemming(token))\n",
        "    header = ' '.join(norm_list).lower()\n",
        "    return header"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VowXQriZdVsb"
      },
      "source": [
        "def get_pairs(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    print(\"reading ... \")\n",
        "    pairs=[]\n",
        "    index = 1\n",
        "    for i in tq(range(len(lines))):\n",
        "        l=lines[i]\n",
        "        tmp1=parse_header(json.loads(l)[\"simple\"])\n",
        "        tmp2=parse_header(json.loads(l)[\"comment\"])\n",
        "#         tmp2=json.loads(l)[\"nl\"]\n",
        "        tmp3=float(json.loads(l)[\"coherence\"])\n",
        "        tmp5=json.loads(l)[\"code\"]\n",
        "        \n",
        "        if len(tmp1.split()) < 351 and len(tmp2.split()) < 351 and len(tmp1.split()) > 2 and len(tmp2.split()) > 2:\n",
        "            pairs.append([tmp1,tmp2,tmp3,index,tmp5])\n",
        "        else:\n",
        "            continue\n",
        "        index += 1\n",
        "    return pairs"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Tk4gpZsraB",
        "outputId": "f7577918-b152-4b7a-f8fa-4a93391a6fe1"
      },
      "source": [
        "test_pairs_nobug = get_pairs(\"ast_test_nobug.json\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2314 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "reading ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2314/2314 [00:06<00:00, 343.53it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WZp_tzNsOZH",
        "outputId": "94c20108-38e8-461d-c182-31d82266c5a4"
      },
      "source": [
        "test_pairs_bug = get_pairs(\"ast_test_bug.json\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 27/27 [00:00<00:00, 322.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "reading ... \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZcSgICRdVse"
      },
      "source": [
        "with open(\"test_pairs_nobug.pkl\", \"wb\") as f:\n",
        "    pickle.dump(test_pairs_nobug, f)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8T9Blp_wXlY"
      },
      "source": [
        "with open(\"test_pairs_bug.pkl\", \"wb\") as f:\n",
        "    pickle.dump(test_pairs_bug, f)"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}